{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Big_data_Project.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"yT7877SMbfW0"},"source":["***Import***"]},{"cell_type":"code","metadata":{"id":"uaU798lez-Fl"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.preprocessing import sequence\n","from tensorflow.keras.optimizers import RMSprop"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kWGs9fWZbuOR"},"source":["***Data Preprocessing***"]},{"cell_type":"code","metadata":{"id":"QbA2pVV50V9B"},"source":["data = pd.read_csv('/content/Dataset.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DMRCKA3j0jY-"},"source":["train = data['Comment']\n","target = data['MOS']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P65lD87HcbpV"},"source":["We need to categorically encode the labels"]},{"cell_type":"code","metadata":{"id":"-i92UUVi3IUx"},"source":["from tensorflow.keras.utils import to_categorical\n","target = to_categorical(target)\n","target = np.array(target).astype('float32')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ii23UnakEGz"},"source":["Preparing our Data \n","\n","We will turn our text into lists of integer indices"]},{"cell_type":"code","metadata":{"id":"j9IDFgHj2CTo"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","samples = train\n","\n","#Create a tokenizer configured to take the 1000 most common words\n","tokenizer = Tokenizer(num_words=1000)\n","#Build the word index\n","tokenizer.fit_on_texts(samples)\n","#Turn strings into lists of integer indices\n","sequences = tokenizer.texts_to_sequences(samples)\n","#get the one-hot binary representations\n","one_hot_results = tokenizer.texts_to_matrix(samples, mode=\"binary\")\n","\n","#Recover the word index\n","word_index = tokenizer.word_index"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9pDIYLA5clIL"},"source":["Data Splitting\n","\n","Training Data: 130\n","\n","Testing Data: 21"]},{"cell_type":"code","metadata":{"id":"rUsoGcMC09gN"},"source":["x_train = sequences[:130]\n","y_train = target[:130]\n","\n","x_test = sequences[130:]\n","y_test = target[130:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BGhN9Hmc0psG"},"source":["max_features = 10000\n","max_len = 500"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RGX8gGhpc2LP"},"source":["We will **Pad** our Data so that they all have the same length, **turn them into an integer tensor of shape (samples, word_indices)**, and then use as the first layer in your network a layer capable of handling such integer tensors(**Embedding layer** in our case)"]},{"cell_type":"code","metadata":{"id":"5I0r6pc14yM4"},"source":["x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n","x_test = sequence.pad_sequences(x_test, maxlen=max_len)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZAOol4ySdTeu"},"source":["Loading & Preparing GloVe for word representation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hpyond6p7n-d","executionInfo":{"status":"ok","timestamp":1633971204125,"user_tz":-60,"elapsed":11872,"user":{"displayName":"Ouerghie Belhassen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0fnkBdN98g4L3BzPbaPHqFlsmwR3nJHvAeXjTvA=s64","userId":"04984451258802739983"}},"outputId":"74346285-f658-418a-87b9-d36b68b39952"},"source":["import os \n","\n","#Parsing the GloVe word-embeddings file\n","\n","embeddings_index = {}\n","f = open( '/content/drive/MyDrive/glove.6B.100d.txt')\n","\n","for line in f:\n","  values = line.split()\n","  word = values[0]\n","  coefs = np.asarray(values[1:], dtype='float32')\n","  embeddings_index[word] = coefs\n","f.close()\n","\n","print('Found %s word vectors.' % len(embeddings_index))\n","\n","#Preparing the GloVe word-embeddings matrix\n","embedding_dim = 100\n","\n","embedding_matrix = np.zeros((max_features, embedding_dim))\n","\n","for word, i in word_index.items():\n","  if i < max_features:\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","      embedding_matrix[i] = embedding_vector"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 400000 word vectors.\n"]}]},{"cell_type":"markdown","metadata":{"id":"YYBGQpnAdOcP"},"source":["***Building the Model***"]},{"cell_type":"code","metadata":{"id":"P15QSpmP1OaQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633971204858,"user_tz":-60,"elapsed":748,"user":{"displayName":"Ouerghie Belhassen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0fnkBdN98g4L3BzPbaPHqFlsmwR3nJHvAeXjTvA=s64","userId":"04984451258802739983"}},"outputId":"dc72bbe8-5af8-474c-e70c-4129422a6711"},"source":["from keras.models import Sequential\n","from keras import layers\n","\n","model = Sequential()\n","model.add(layers.Embedding(max_features, 100, input_length=max_len))\n","\n","\n","model.add(layers.Conv1D(32, 7, activation='relu'))\n","model.add(layers.MaxPooling1D(5))\n","model.add(layers.Dropout(0.2))\n","model.add(layers.Conv1D(32, 7,activation='relu'))\n","\n","model.add(layers.GRU(32,\n","                     dropout=0.2,\n","                     recurrent_dropout=0.2))\n","\n","model.add(layers.Dense(6, activation=\"softmax\"))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"]}]},{"cell_type":"markdown","metadata":{"id":"BsqRbQl8edWb"},"source":["Our model is ready. Let's freeze the Embedding layer to avoid loosing the learned information"]},{"cell_type":"code","metadata":{"id":"Pq5rVyuq8n1U"},"source":["model.layers[0].set_weights([embedding_matrix])\n","model.layers[0].trainable = False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HVNQierwi9Hq"},"source":["***Training***"]},{"cell_type":"code","metadata":{"id":"rprHkSnn2wK6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633971314649,"user_tz":-60,"elapsed":109798,"user":{"displayName":"Ouerghie Belhassen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0fnkBdN98g4L3BzPbaPHqFlsmwR3nJHvAeXjTvA=s64","userId":"04984451258802739983"}},"outputId":"25983079-3667-4b48-bc1f-aa018cf2a12c"},"source":["model.summary()\n","\n","model.compile(optimizer=RMSprop(learning_rate=1e-4),\n","              loss='categorical_crossentropy',\n","              metrics=['acc'])\n","\n","history = model.fit(x_train, y_train,\n","                    epochs=20,\n","                    batch_size=16)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 500, 100)          1000000   \n","_________________________________________________________________\n","conv1d (Conv1D)              (None, 494, 32)           22432     \n","_________________________________________________________________\n","max_pooling1d (MaxPooling1D) (None, 98, 32)            0         \n","_________________________________________________________________\n","dropout (Dropout)            (None, 98, 32)            0         \n","_________________________________________________________________\n","conv1d_1 (Conv1D)            (None, 92, 32)            7200      \n","_________________________________________________________________\n","gru (GRU)                    (None, 32)                6336      \n","_________________________________________________________________\n","dense (Dense)                (None, 6)                 198       \n","=================================================================\n","Total params: 1,036,166\n","Trainable params: 36,166\n","Non-trainable params: 1,000,000\n","_________________________________________________________________\n","Epoch 1/20\n","9/9 [==============================] - 9s 565ms/step - loss: 1.7902 - acc: 0.2615\n","Epoch 2/20\n","9/9 [==============================] - 5s 553ms/step - loss: 1.7561 - acc: 0.3308\n","Epoch 3/20\n","9/9 [==============================] - 5s 569ms/step - loss: 1.7306 - acc: 0.4077\n","Epoch 4/20\n","9/9 [==============================] - 5s 575ms/step - loss: 1.7058 - acc: 0.4000\n","Epoch 5/20\n","9/9 [==============================] - 5s 574ms/step - loss: 1.6867 - acc: 0.4462\n","Epoch 6/20\n","9/9 [==============================] - 5s 552ms/step - loss: 1.6759 - acc: 0.4231\n","Epoch 7/20\n","9/9 [==============================] - 5s 553ms/step - loss: 1.6459 - acc: 0.5077\n","Epoch 8/20\n","9/9 [==============================] - 5s 575ms/step - loss: 1.6359 - acc: 0.4923\n","Epoch 9/20\n","9/9 [==============================] - 5s 593ms/step - loss: 1.6117 - acc: 0.4846\n","Epoch 10/20\n","9/9 [==============================] - 5s 589ms/step - loss: 1.5860 - acc: 0.5000\n","Epoch 11/20\n","9/9 [==============================] - 5s 578ms/step - loss: 1.5713 - acc: 0.5000\n","Epoch 12/20\n","9/9 [==============================] - 5s 591ms/step - loss: 1.5553 - acc: 0.5154\n","Epoch 13/20\n","9/9 [==============================] - 5s 604ms/step - loss: 1.5514 - acc: 0.4846\n","Epoch 14/20\n","9/9 [==============================] - 5s 588ms/step - loss: 1.5299 - acc: 0.5231\n","Epoch 15/20\n","9/9 [==============================] - 5s 583ms/step - loss: 1.5085 - acc: 0.5385\n","Epoch 16/20\n","9/9 [==============================] - 5s 606ms/step - loss: 1.5111 - acc: 0.5077\n","Epoch 17/20\n","9/9 [==============================] - 5s 594ms/step - loss: 1.4990 - acc: 0.5154\n","Epoch 18/20\n","9/9 [==============================] - 5s 596ms/step - loss: 1.4737 - acc: 0.5154\n","Epoch 19/20\n","9/9 [==============================] - 5s 582ms/step - loss: 1.4713 - acc: 0.4846\n","Epoch 20/20\n","9/9 [==============================] - 5s 603ms/step - loss: 1.4539 - acc: 0.5077\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1U_v0qpeK-5I","executionInfo":{"status":"ok","timestamp":1633971315068,"user_tz":-60,"elapsed":451,"user":{"displayName":"Ouerghie Belhassen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj0fnkBdN98g4L3BzPbaPHqFlsmwR3nJHvAeXjTvA=s64","userId":"04984451258802739983"}},"outputId":"eea2d708-f40c-46cb-b875-2dba7161d16b"},"source":["resultat= model.evaluate(x_test,y_test)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 1s 706ms/step - loss: 1.3538 - acc: 0.6190\n"]}]},{"cell_type":"markdown","metadata":{"id":"BMZot7aojAYF"},"source":["***We reached ~62% accuracy on the testing data***"]}]}